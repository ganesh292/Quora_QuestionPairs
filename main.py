# -*- coding: utf-8 -*-
"""Architecture+Embeddings_Ganesh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10a_xNH0kyI0xMAAor1mjn8_Dp9E7WFJ0
"""
# avoid decoding problems
import sys
import os 
import pandas as pd
import numpy as np
from tqdm import tqdm
from time import time


import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import gensim
from gensim.models import Word2Vec, FastText
from fse.models import Sentence2Vec
# Make sure, that the fast version of fse is available!
from fse.models.sentence2vec import CY_ROUTINES
assert CY_ROUTINES

from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize


from keras import backend as K
import tensorflow as tf
import tensorflow_hub as hub
from keras.models import Model, Sequential
from keras.layers import Input, Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers, Add, concatenate, Layer,Lambda
from keras.optimizers import RMSprop, SGD, Adam

# Network Architecture

from keras.models import Sequential, Model
from keras.layers import Conv1D , MaxPooling1D, Flatten,Dense,Input,Lambda
from keras.layers import LSTM, Concatenate, Dropout
from keras.layers import Dense
from keras.layers import TimeDistributed
from keras.layers import Bidirectional
from keras import backend as K
import tensorflow as tf

import numpy as np

import torch
from models import InferSent

# import matplotlib as mpl
# # %matplotlib inline
# from matplotlib import pyplot as plt
# from keras.utils import plot_model 
# from IPython.display import Image
# plot_model(net, to_file='model_upgraded_1.png', show_layer_names=True)
# Image('model_upgraded_1.png')
# from google.colab import files
# files.download( "model_upgraded_1.png" )


modelg = infersent_glove()

#Word Embeddings
def infersent_glove():
    #Set Model for InferSent+Glove
    V = 1
    MODEL_PATH = '/tmp/GloVe/encoder/infersent%s.pkl' % V
    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,
                    'pool_type': 'max', 'dpout_model': 0.0, 'version': V}
    modelg = InferSent(params_model)
    modelg.load_state_dict(torch.load(MODEL_PATH))
    # Keep it on CPU or put it on GPU
    use_cuda = True
    modelg = modelg.cuda() if use_cuda else model

    # If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.
    W2V_PATH = '/tmp/GloVe/glove.840B.300d.txt' if V == 1 else 'fastText/crawl-300d-2M.vec'
    modelg.set_w2v_path(W2V_PATH)
    # Load embeddings of K most frequent words
    modelg.build_vocab_k_words(K=100000)
    return modelg


def get_fastext(sentences_tok):
  print("Training FastText model ...\n")
  model = FastText(size=324, window=10, min_count=1)  # instantiate
  model.build_vocab(sentences_tok)
  model.train(sentences=sentences_tok,total_examples=len(sentences_tok),epochs=5)  # train
  se = Sentence2Vec(model)
  ft_embeddings = se.train(sentences_tok)
  return ft_embeddings
  

def get_w2v(sentences_tok):
  #Word2Vec Embeddings
  print("Training W2v model ...\n")
  w2v_model = Word2Vec(sentences_tok,size=324,window=10, min_count=1)
  se = Sentence2Vec(w2v_model)
  w2v_embeddings = se.train(sentences_tok)
  return w2v_embeddings

def get_glove(sentences):
  print("Training glove+infersent model ...\n")
  embeddings = modelg.encode(sentences, bsize=128, tokenize=False, verbose=True)
  pca = PCA(n_components=324) #reduce down to 50 dim
  glove_embeddings = pca.fit_transform(embeddings)
  return glove_embeddings


def create_base_network_cnn(input_dimensions):
  
  input  = Input(shape=(input_dimensions[0],input_dimensions[1]))
  conv1  = Conv1D(filters=32,kernel_size=8,strides=1,activation = 'relu',name='conv1')(input)
  pool1  = MaxPooling1D(pool_size=1,strides=1,name='pool1')(conv1)
  conv2  = Conv1D(filters=64,kernel_size=6,strides=1,activation='relu',name='conv2')(pool1)
  pool2  = MaxPooling1D(pool_size=1,strides=1,name='pool2')(conv2)
  conv3  = Conv1D(filters=128,kernel_size=4,strides=1,activation='relu',name='conv3')(pool2)
  flat   = Flatten(name='flat_cnn')(conv3)
  dense  = Dense(376,name='dense_cnn')(flat)
   
  model  = Model(input=input,output=dense)
  return model

def create_base_network_lstm(input_dimensions):
  input = Input(shape=(input_dimensions[0],1))
  layer1 = LSTM(20, return_sequences=True,activation='relu',name='lstm_1')(input)
  layer2 = LSTM(20,return_sequences=False,activation='relu',name='lstm_2')(layer1)
  dense = Dense(376,name='dense_lstm')(layer2)
  
  model = Model(input=input,output=dense)
  return model
  

def dense_network(features):
  input = Input(shape=(features[0],features[1]))
  #x = Flatten()(features)
  d1 = Dense(128, activation='relu')(input)
  drop1 = Dropout(0.1)(d1)
  d2 = Dense(128, activation='relu')(drop1)
  drop2 = Dropout(0.1)(d2)
  d3 = Dense(1, activation='relu')(drop2)
  model = Model(input = input,output=d3)
  return model
  

def euclidean_distance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))

def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)


def create_network(input_dimensions):
  base_network_lstm_1 = create_base_network_lstm(input_dimensions)
  base_network_lstm_2 = create_base_network_lstm(input_dimensions)
  base_network_lstm_3 = create_base_network_lstm(input_dimensions)
  base_network_cnn = create_base_network_cnn(input_dimensions)
  
  
  input_a_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))
  input_b_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))
  
  input_a_lstm_1 = Input(shape=(input_dimensions[0],1))
  input_b_lstm_1 = Input(shape=(input_dimensions[0],1))
                         
  input_a_lstm_2 = Input(shape=(input_dimensions[0],1))
  input_b_lstm_2 = Input(shape=(input_dimensions[0],1))
                         
  input_a_lstm_3 = Input(shape=(input_dimensions[0],1))
  input_b_lstm_3 = Input(shape=(input_dimensions[0],1))
  
  
  #feature_1 = Input(shape=(1,))
  #feature_2 = Input(shape=(1,))

  
  # CNN with 3 channel embedding
  inter_a_cnn = base_network_cnn(input_a_cnn)
  inter_b_cnn = base_network_cnn(input_b_cnn)
  
  # LSTM with embedding 1
  inter_a_lstm_1 = base_network_lstm_1(input_a_lstm_1)
  inter_b_lstm_1 = base_network_lstm_1(input_b_lstm_1)
  
  # LSTM with embedding 2
  inter_a_lstm_2 = base_network_lstm_2(input_a_lstm_2)
  inter_b_lstm_2 = base_network_lstm_2(input_b_lstm_2)
  
  # LSTM with embedding 3
  inter_a_lstm_3 = base_network_lstm_3(input_a_lstm_3)
  inter_b_lstm_3 = base_network_lstm_3(input_b_lstm_3)
  
  
  d_cnn = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_cnn, inter_b_cnn])
  d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])
  d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])
  d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])
  
  
  feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_1,d_lstm_2,d_lstm_3])
  d1 = Dense(128, activation='relu')(feature_set)
  drop1 = Dropout(0.1)(d1)
  d2 = Dense(128, activation='relu')(drop1)
  drop2 = Dropout(0.1)(d2)
  d3 = Dense(1, activation='relu')(drop2)
  
  
  #value = dense_network(feature_set)
  
  model = Model(input=[input_a_cnn, input_b_cnn , input_a_lstm_1, input_b_lstm_1, input_a_lstm_2, input_b_lstm_2, input_a_lstm_3, input_b_lstm_3], output=d3)
  print("Model Architecture Designed")
  return model
  


def main():
    #Get Dataset
  df_sub = pd.read_csv('data_balanced.csv')
  data_features = pd.read_csv("quora_features_balanced.csv")
  print('Shape of Dataset',df_sub.shape)
  print('Shape of Features',data_features.iloc[:,3].values.shape)
  

  df_sub['question1'] = df_sub['question1'].apply(lambda x: str(x))
  df_sub['question2'] = df_sub['question2'].apply(lambda x: str(x))
  q1sents = list(df_sub['question1'])
  q2sents = list(df_sub['question2'])
  tokenized_q1sents = [word_tokenize(i) for i in list(df_sub['question1'])]
  tokenized_q2sents = [word_tokenize(i) for i in list(df_sub['question2'])]

  #Compute Embeddings for Q1 Pair
  ft_emb_q1 = get_fastext(tokenized_q1sents)
  w2v_emb_q1 = get_w2v(tokenized_q1sents)
  glove_emb_q1 = get_glove(q1sents)


  #Compute Embeddings for Q2 Pair
  ft_emb_q2 = get_fastext(tokenized_q2sents)
  w2v_emb_q2 = get_w2v(tokenized_q2sents)
  glove_emb_q2 = get_glove(q2sents)

  #Preparing Data for Training Network
  df_sub = df_sub.reindex(np.random.permutation(df_sub.index))

  # set number of train and test instances
  num_train = int(df_sub.shape[0] * 0.70)
  num_val = int(df_sub.shape[0] * 0.10)
  num_test = df_sub.shape[0] - num_train - num_val               
  print("Number of training pairs: %i"%(num_train))
  print("Number of Validation pairs: %i"%(num_val))
  print("Number of testing pairs: %i"%(num_test))


  # init data data arrays
  X_train_cnn_a = np.zeros([num_train, 324, 3])
  X_test_cnn_a  = np.zeros([num_test, 324, 3])
  X_val_cnn_a  = np.zeros([num_val, 324, 3])

  X_train_cnn_b = np.zeros([num_train, 324, 3])
  X_test_cnn_b  = np.zeros([num_test, 324, 3])
  X_val_cnn_b  = np.zeros([num_val, 324, 3])

  Y_train = np.zeros([num_train]) 
  Y_test = np.zeros([num_test])
  Y_val = np.zeros([num_val]) 

  # format data 
  # b = [a[None,:] for a in list(df_sub['q1_feats'].values)]
  # q1_feats = np.concatenate(b, axis=0)

  # b = [a[None,:] for a in list(df_sub['q2_feats'].values)]
  # q2_feats = np.concatenate(b, axis=0)


  num_val = num_train + int(df_sub.shape[0] * 0.10)
  # fill data arrays with features
  X_train_cnn_a[:,:,0] = ft_emb_q1[:num_train]
  X_train_cnn_a[:,:,1] = w2v_emb_q1[:num_train]
  X_train_cnn_a[:,:,2] = glove_emb_q1[:num_train]

  X_train_cnn_b[:,:,0] = ft_emb_q2[:num_train]
  X_train_cnn_b[:,:,1] = w2v_emb_q2[:num_train]
  X_train_cnn_b[:,:,2] = glove_emb_q2[:num_train]


  Y_train = df_sub[:num_train]['is_duplicate'].values

  X_val_cnn_a[:,:,0] = ft_emb_q1[num_train:num_val]
  X_val_cnn_a[:,:,1] = w2v_emb_q1[num_train:num_val]
  X_val_cnn_a[:,:,2] = glove_emb_q1[num_train:num_val]

  X_val_cnn_b[:,:,0] = ft_emb_q2[num_train:num_val]
  X_val_cnn_b[:,:,1] = w2v_emb_q2[num_train:num_val]
  X_val_cnn_b[:,:,2] = glove_emb_q2[num_train:num_val]


  Y_val = df_sub[num_train:num_val]['is_duplicate'].values

              
  X_test_cnn_a[:,:,0] = ft_emb_q1[num_val:]
  X_test_cnn_a[:,:,1] = w2v_emb_q1[num_val:]
  X_test_cnn_a[:,:,2] = glove_emb_q1[num_val:]

  X_test_cnn_b[:,:,0] = ft_emb_q2[num_val:]
  X_test_cnn_b[:,:,1] = w2v_emb_q2[num_val:]
  X_test_cnn_b[:,:,2] = glove_emb_q2[num_val:]

  Y_test = df_sub[num_val:]['is_duplicate'].values


  print("Input Shapes")
  print("CNN A:")
  print(X_train_cnn_a.shape,X_val_cnn_a.shape,X_test_cnn_a.shape)
  print("CNN B:")
  print(X_train_cnn_b.shape,X_val_cnn_b.shape,X_test_cnn_b.shape)
  print("Single Channel Shapes for Others")


  print("Label's Shape")
  print(Y_train.shape,Y_val.shape,Y_test.shape)


 
  
  
  #############Pipeline Starts################
  net = create_network([324,3])
  optimizer = Adam(lr=0.001)
  net.compile(loss="binary_crossentropy", optimizer=optimizer,metrics=['accuracy'])
  print(net.summary())


  # Training Sets for LSTMS

  X_intera_train_1 = X_train_cnn_a[:,:,0]
  X_interb_train_1 = X_train_cnn_b[:,:,0]
  X_train_lstm1_a = X_intera_train_1[:,:,np.newaxis]
  X_train_lstm1_b = X_intera_train_1[:,:,np.newaxis]

  X_intera_train_2 = X_train_cnn_a[:,:,1]
  X_interb_train_2 = X_train_cnn_b[:,:,1]
  X_train_lstm2_a = X_intera_train_2[:,:,np.newaxis]
  X_train_lstm2_b = X_intera_train_2[:,:,np.newaxis]

  X_intera_train_3 = X_train_cnn_a[:,:,2]
  X_interb_train_3 = X_train_cnn_b[:,:,2]
  X_train_lstm3_a = X_intera_train_3[:,:,np.newaxis]
  X_train_lstm3_b = X_intera_train_3[:,:,np.newaxis]

  # Validation Sets for LSTMS

  X_intera_val_1 = X_val_cnn_a[:,:,0]
  X_interb_val_1 = X_val_cnn_b[:,:,0]
  X_val_lstm1_a = X_intera_val_1[:,:,np.newaxis]
  X_val_lstm1_b = X_intera_val_1[:,:,np.newaxis]

  X_intera_val_2 = X_val_cnn_a[:,:,1]
  X_interb_val_2 = X_val_cnn_b[:,:,1]
  X_val_lstm2_a = X_intera_val_2[:,:,np.newaxis]
  X_val_lstm2_b = X_intera_val_2[:,:,np.newaxis]

  X_intera_val_3 = X_val_cnn_a[:,:,2]
  X_interb_val_3 = X_val_cnn_b[:,:,2]
  X_val_lstm3_a = X_intera_val_3[:,:,np.newaxis]
  X_val_lstm3_b = X_intera_val_3[:,:,np.newaxis]

  for epoch in range(1):
      net.fit([X_train_cnn_a, X_train_cnn_b, X_train_lstm1_a, X_train_lstm1_b,
              X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b], 
              Y_train,
            validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,
                            X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b]
                            , Y_val),
            batch_size=128, nb_epoch=1, shuffle=True, )
  return 0


if __name__== "__main__":
  main()





