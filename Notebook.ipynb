{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Architecture+Embeddings_Ganesh.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/10a_xNH0kyI0xMAAor1mjn8_Dp9E7WFJ0\n",
    "\"\"\"\n",
    "# avoid decoding problems\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "# from bert_serving.client import BertClient\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec, FastText\n",
    "# from fse.models import Sentence2Vec\n",
    "# # Make sure, that the fast version of fse is available!\n",
    "# from fse.models.sentence2vec import CY_ROUTINES\n",
    "# assert CY_ROUTINES\n",
    "\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers, Add, concatenate, Layer,Lambda\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "# Network Architecture\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D , MaxPooling1D, Flatten,Dense,Input,Lambda\n",
    "from keras.layers import LSTM, Concatenate, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "# import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# from models import InferSent\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def l1_reg(weight_matrix):\n",
    "    return 0.01 * K.sum(K.abs(weight_matrix))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_base_network_cnn(input_dimensions):\n",
    "\n",
    "    input  = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
    "    conv1  = Conv1D(filters=32,kernel_size=8,strides=1,activation = 'relu',name='conv1')(input)\n",
    "    pool1  = MaxPooling1D(pool_size=1,strides=1,name='pool1')(conv1)\n",
    "    conv2  = Conv1D(filters=64,kernel_size=6,strides=1,activation='relu',name='conv2')(pool1)\n",
    "    pool2  = MaxPooling1D(pool_size=1,strides=1,name='pool2')(conv2)\n",
    "    conv3  = Conv1D(filters=128,kernel_size=4,strides=1,activation='relu',name='conv3')(pool2)\n",
    "    flat   = Flatten(name='flat_cnn')(conv3)\n",
    "    # dense  = Dense(376,name='dense_cnn')(flat)\n",
    "    dense  = Dense(100,name='dense_cnn')(flat)\n",
    "\n",
    "    model  = Model(input=input,output=dense)\n",
    "    return model\n",
    "\n",
    "def create_base_network_lstm(input_dimensions):\n",
    "    input = Input(shape=(input_dimensions[0],1))\n",
    "    layer1 = LSTM(20, return_sequences=True,activation='relu',name='lstm_1')(input)\n",
    "    b3 = BatchNormalization()(layer1)\n",
    "    layer2 = LSTM(20,return_sequences=False,activation='relu',name='lstm_2')(b3)\n",
    "    b3 = BatchNormalization()(layer2)\n",
    "    dense = Dense(100,name='dense_lstm')(b3)\n",
    "\n",
    "    model = Model(input=input,output=dense)\n",
    "    return model\n",
    "  \n",
    "\n",
    "def dense_network(features):\n",
    "    input = Input(shape=(features[0],features[1]))\n",
    "    #x = Flatten()(features)\n",
    "    d1 = Dense(128, activation='relu')(input)\n",
    "    drop1 = Dropout(0.1)(d1)\n",
    "    b1 = BatchNormalization()(drop1)\n",
    "    d2 = Dense(128, activation='relu')(b1)\n",
    "    drop2 = Dropout(0.1)(d2)\n",
    "    b2 = BatchNormalization()(drop2)\n",
    "    d3 = Dense(1, activation='relu')(b2)\n",
    "    model = Model(input = input,output=d3)\n",
    "    return model\n",
    "  \n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "    \n",
    "def add_features():\n",
    "#   if sys.argv[1] == \"b\":\n",
    "    data_features = pd.read_csv(\"quora_features_BERT_balanced.csv\")\n",
    "#   else:\n",
    "#     data_features = pd.read_csv(\"quora_features_BERT_unbalanced.csv\")\n",
    "  \n",
    "    features = data_features.drop(['question1', 'question2', 'is_duplicate','jaccard_distance'],axis=1).values\n",
    "    print('Shape of Features added',features.shape)\n",
    "    return features\n",
    "\n",
    "def create_network(input_dimensions,num_features):\n",
    "\n",
    "    # #Fasttext\n",
    "    base_network_lstm_1 = create_base_network_lstm(input_dimensions)\n",
    "    input_a_lstm_1 = Input(shape=(input_dimensions[0],1))\n",
    "    input_b_lstm_1 = Input(shape=(input_dimensions[0],1))\n",
    "    # LSTM with embedding 1\n",
    "    inter_a_lstm_1 = base_network_lstm_1(input_a_lstm_1)\n",
    "    inter_b_lstm_1 = base_network_lstm_1(input_b_lstm_1)\n",
    "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
    "\n",
    "\n",
    "    #W2V\n",
    "    base_network_lstm_2 = create_base_network_lstm(input_dimensions)\n",
    "    input_a_lstm_2 = Input(shape=(input_dimensions[0],1))\n",
    "    input_b_lstm_2 = Input(shape=(input_dimensions[0],1))\n",
    "    # LSTM with embedding 2\n",
    "    inter_a_lstm_2 = base_network_lstm_2(input_a_lstm_2)\n",
    "    inter_b_lstm_2 = base_network_lstm_2(input_b_lstm_2)\n",
    "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
    "\n",
    "\n",
    "    #Glove\n",
    "    base_network_lstm_3 = create_base_network_lstm(input_dimensions)\n",
    "    input_a_lstm_3 = Input(shape=(input_dimensions[0],1))\n",
    "    input_b_lstm_3 = Input(shape=(input_dimensions[0],1))\n",
    "    # LSTM with embedding 3\n",
    "    inter_a_lstm_3 = base_network_lstm_3(input_a_lstm_3)\n",
    "    inter_b_lstm_3 = base_network_lstm_3(input_b_lstm_3)\n",
    "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
    "\n",
    "    #BERT\n",
    "    base_network_lstm_4 = create_base_network_lstm([768,1])\n",
    "    input_a_lstm_4 = Input(shape=(768,1))\n",
    "    input_b_lstm_4 = Input(shape=(768,1))\n",
    "    # LSTM with embedding 3\n",
    "    inter_a_lstm_4 = base_network_lstm_4(input_a_lstm_4)\n",
    "    inter_b_lstm_4 = base_network_lstm_4(input_b_lstm_4)\n",
    "\n",
    "\n",
    "    #CNN\n",
    "    base_network_cnn = create_base_network_cnn(input_dimensions)\n",
    "    # CNN with 3 channel embedding\n",
    "    input_a_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
    "    input_b_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
    "    inter_a_cnn = base_network_cnn(input_a_cnn)\n",
    "    inter_b_cnn = base_network_cnn(input_b_cnn)\n",
    "\n",
    "\n",
    "\n",
    "    d_cnn = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_cnn, inter_b_cnn])\n",
    "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
    "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
    "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
    "    d_lstm_4 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_4, inter_b_lstm_4])\n",
    "\n",
    "    # Additional Features from Thakur (BERT)\n",
    "    features = Input(shape=(num_features,))\n",
    "\n",
    "    #BERT itself\n",
    "    features_b = Input(shape=(768,))\n",
    "\n",
    "\n",
    "    #Concatenation of Features\n",
    "    feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_1,d_lstm_2,d_lstm_3,d_lstm_4,features,features_b])\n",
    "    # feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_1,d_lstm_2,d_lstm_3,features,features_b])\n",
    "    # feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_4,features,features_b])\n",
    "    # feature_set = add_features(feature_set)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x = Flatten()(features)\n",
    "    d1 = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.1))(feature_set)\n",
    "    drop1 = Dropout(0.3)(d1)\n",
    "    b1 = BatchNormalization()(drop1)\n",
    "    d2 = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.1))(b1)\n",
    "    drop2 = Dropout(0.3)(d2)\n",
    "    b2 = BatchNormalization()(drop2)\n",
    "    d3 = Dense(1, activation='relu',kernel_regularizer=regularizers.l2(0.1))(b2)\n",
    "\n",
    "\n",
    "    # model = Model(input=[input_a_cnn, input_b_cnn , input_a_lstm_4, input_b_lstm_4,features,features_b], output=d3)\n",
    "    # model = Model(input=[input_a_cnn, input_b_cnn , input_a_lstm_1, input_b_lstm_1, input_a_lstm_2, input_b_lstm_2, input_a_lstm_3, input_b_lstm_3,features,features_b], output=d3)\n",
    "    model = Model(input=[input_a_cnn, input_b_cnn,input_a_lstm_1, input_b_lstm_1, input_a_lstm_2, input_b_lstm_2, input_a_lstm_3, input_b_lstm_3,input_a_lstm_4, input_b_lstm_4,features,features_b], output=d3)\n",
    "\n",
    "    print(\"Model Architecture Designed\")\n",
    "    return model\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = pd.read_csv('data_unbalanced.csv')\n",
    "df_sub = pd.read_csv('data_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['question1'] = df_sub['question1'].apply(lambda x: str(x))\n",
    "df_sub['question2'] = df_sub['question2'].apply(lambda x: str(x))\n",
    "q1sents = list(df_sub['question1'])\n",
    "q2sents = list(df_sub['question2'])\n",
    "tokenized_q1sents = [word_tokenize(i) for i in list(df_sub['question1'])]\n",
    "tokenized_q2sents = [word_tokenize(i) for i in list(df_sub['question2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emb_q1 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/word2vec/w2vec_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "w2v_emb_q2 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/word2vec/w2vec_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "w2v_emb_q1 = np.delete(w2v_emb_q1, 0, 1)\n",
    "w2v_emb_q2 = np.delete(w2v_emb_q2, 0, 1)\n",
    "print('Loading Embeddings fastext')\n",
    "ft_emb_q1 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/fastext/fastext_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "ft_emb_q2 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/fastext/fastext_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "ft_emb_q1 = np.delete(ft_emb_q1, 0, 1)\n",
    "ft_emb_q2 = np.delete(ft_emb_q2,0, 1)\n",
    "print('Loading Embeddings glove')\n",
    "glove_emb_q1 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/glove/glove_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "glove_emb_q2 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/glove/glove_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "glove_emb_q1 = np.delete(glove_emb_q1,0, 1)\n",
    "glove_emb_q2 = np.delete(glove_emb_q2, 0, 1)\n",
    "print('Loading Embeddings BERT')\n",
    "bert_q = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_qpair_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q = np.delete(bert_q,0,1)\n",
    "bert_q1 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q1 = np.delete(bert_q1,0,1)\n",
    "print('Loading Embeddings BERTQ2')\n",
    "bert_q2 = genfromtxt('/tmp/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q2 = np.delete(bert_q2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings W2vec\n",
      "Loading Embeddings fastext\n",
      "Loading Embeddings glove\n"
     ]
    }
   ],
   "source": [
    "print('Loading Embeddings W2vec')\n",
    "w2v_emb_q1 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/word2vec/w2vec_q1_balanced.csv', delimiter=',',skip_header=1)\n",
    "w2v_emb_q2 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/word2vec/w2vec_q2_balanced.csv', delimiter=',',skip_header=1)\n",
    "\n",
    "print('Loading Embeddings fastext')\n",
    "ft_emb_q1 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/fastext/fastext_q1_balanced.csv', delimiter=',',skip_header=1)\n",
    "ft_emb_q2 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/fastext/fastext_q2_balanced.csv', delimiter=',',skip_header=1)\n",
    "\n",
    "print('Loading Embeddings glove')\n",
    "glove_emb_q1 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/glove/glove_q1_balanced.csv', delimiter=',',skip_header=1)\n",
    "glove_emb_q2 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/glove/glove_q2_balanced.csv', delimiter=',',skip_header=1)\n",
    "w2v_emb_q1 = np.delete(w2v_emb_q1, 0, 1)\n",
    "w2v_emb_q2 = np.delete(w2v_emb_q2, 0, 1)\n",
    "\n",
    "ft_emb_q1 = np.delete(ft_emb_q1, 0, 1)\n",
    "ft_emb_q2 = np.delete(ft_emb_q2, 0, 1)\n",
    "\n",
    "glove_emb_q1 = np.delete(glove_emb_q1, 0, 1)\n",
    "glove_emb_q2 = np.delete(glove_emb_q2, 0, 1)\n",
    "print('Loading Embeddings BERT')\n",
    "bert_q = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/bert/bert_qpair_balanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q = np.delete(bert_q,0,1)\n",
    "print('Loading Embeddings BERTQ1')\n",
    "bert_q1 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/bert/bert_q1_balanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q1 = np.delete(bert_q1,0,1)\n",
    "print('Loading Embeddings BERTQ2')\n",
    "bert_q2 = genfromtxt('C:/Users/HP/Documents/Ganesh_MSCI/balanced_Embeddings/bert/bert_q2_balanced.csv', delimiter=',',skip_header=1)\n",
    "bert_q2 = np.delete(bert_q2,0,1)\n",
    "\n",
    "\n",
    "#Features Formed from BERT\n",
    "features = add_features()\n",
    "# set number of train and test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(df_sub.shape[0] * 0.70)\n",
    "num_val = int(df_sub.shape[0] * 0.10)\n",
    "num_test = df_sub.shape[0] - num_train - num_val \n",
    "\n",
    "# total = np.arange(df_sub.shape[0])\n",
    "# num_train1,num_test = train_test_split(total,test_size=0.1,random_state=33)\n",
    "# num_train,num_val = train_test_split(num_train1,test_size=1/9)\n",
    "\n",
    "print(\"Number of training pairs: %i\"%(num_train))\n",
    "print(\"Number of Validation pairs: %i\"%(num_val))\n",
    "print(\"Number of testing pairs: %i\"%(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data data arrays\n",
    "X_train_cnn_a = np.zeros([num_train, 324, 3])\n",
    "X_test_cnn_a  = np.zeros([num_test, 324, 3])\n",
    "X_val_cnn_a  = np.zeros([num_val, 324, 3])\n",
    "\n",
    "X_train_cnn_b = np.zeros([num_train, 324, 3])\n",
    "X_test_cnn_b  = np.zeros([num_test, 324, 3])\n",
    "X_val_cnn_b  = np.zeros([num_val, 324, 3])\n",
    "\n",
    "Y_train = np.zeros([num_train]) \n",
    "Y_test = np.zeros([num_test])\n",
    "Y_val = np.zeros([num_val]) \n",
    "\n",
    "\n",
    "#Labels\n",
    "Y_train = df_sub['is_duplicate'].values[num_train]\n",
    "Y_val = df_sub['is_duplicate'].values[num_val]\n",
    "Y_test = df_sub['is_duplicate'].values[num_val]\n",
    "\n",
    "\n",
    "\n",
    "num_val = num_train + int(df_sub.shape[0] * 0.10)\n",
    "# fill data arrays with features\n",
    "X_train_cnn_a[:,:,0] = ft_emb_q1[:num_train]\n",
    "X_train_cnn_a[:,:,1] = w2v_emb_q1[:num_train]\n",
    "X_train_cnn_a[:,:,2] = glove_emb_q1[:num_train]\n",
    "\n",
    "X_train_cnn_b[:,:,0] = ft_emb_q2[:num_train]\n",
    "X_train_cnn_b[:,:,1] = w2v_emb_q2[:num_train]\n",
    "X_train_cnn_b[:,:,2] = glove_emb_q2[:num_train]\n",
    "\n",
    "features_train = features[:num_train]\n",
    "features_b_train = bert_q[:num_train]\n",
    "Y_train = df_sub[:num_train]['is_duplicate'].values\n",
    "\n",
    "X_val_cnn_a[:,:,0] = ft_emb_q1[num_train:num_val]\n",
    "X_val_cnn_a[:,:,1] = w2v_emb_q1[num_train:num_val]\n",
    "X_val_cnn_a[:,:,2] = glove_emb_q1[num_train:num_val]\n",
    "\n",
    "X_val_cnn_b[:,:,0] = ft_emb_q2[num_train:num_val]\n",
    "X_val_cnn_b[:,:,1] = w2v_emb_q2[num_train:num_val]\n",
    "X_val_cnn_b[:,:,2] = glove_emb_q2[num_train:num_val]\n",
    "\n",
    "features_val = features[num_train:num_val]\n",
    "features_b_val = bert_q[num_train:num_val]\n",
    "Y_val = df_sub[num_train:num_val]['is_duplicate'].values\n",
    "\n",
    "\n",
    "X_test_cnn_a[:,:,0] = ft_emb_q1[num_val:]\n",
    "X_test_cnn_a[:,:,1] = w2v_emb_q1[num_val:]\n",
    "X_test_cnn_a[:,:,2] = glove_emb_q1[num_val:]\n",
    "\n",
    "X_test_cnn_b[:,:,0] = ft_emb_q2[num_val:]\n",
    "X_test_cnn_b[:,:,1] = w2v_emb_q2[num_val:]\n",
    "X_test_cnn_b[:,:,2] = glove_emb_q2[num_val:]\n",
    "features_test = features[num_val:]\n",
    "features_b_test = bert_q[num_val:]\n",
    "Y_test = df_sub[num_val:]['is_duplicate'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############Pipeline Starts################\n",
    "net = create_network([324,3],25)\n",
    "optimizer = Adam(lr=0.001)\n",
    "net.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=['accuracy'])\n",
    "print(net.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Sets for LSTM1\n",
    "X_intera_train_1 = X_train_cnn_a[:,:,0]\n",
    "X_interb_train_1 = X_train_cnn_b[:,:,0]\n",
    "X_train_lstm1_a = X_intera_train_1[:,:,np.newaxis]\n",
    "X_train_lstm1_b = X_interb_train_1[:,:,np.newaxis]\n",
    "\n",
    "X_intera_val_1 = X_val_cnn_a[:,:,0]\n",
    "X_interb_val_1 = X_val_cnn_b[:,:,0]\n",
    "X_val_lstm1_a = X_intera_val_1[:,:,np.newaxis]\n",
    "X_val_lstm1_b = X_interb_val_1[:,:,np.newaxis]\n",
    "\n",
    "X_intera_test_1 = X_test_cnn_a[:,:,0]\n",
    "X_interb_test_1 = X_test_cnn_b[:,:,0]\n",
    "X_test_lstm1_a = X_intera_test_1[:,:,np.newaxis]\n",
    "X_test_lstm1_b = X_interb_test_1[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "# Sets for LSTM2\n",
    "X_intera_train_2 = X_train_cnn_a[:,:,1]\n",
    "X_interb_train_2 = X_train_cnn_b[:,:,1]\n",
    "X_train_lstm2_a = X_intera_train_2[:,:,np.newaxis]\n",
    "X_train_lstm2_b = X_interb_train_2[:,:,np.newaxis]\n",
    "\n",
    "X_intera_val_2 = X_val_cnn_a[:,:,1]\n",
    "X_interb_val_2 = X_val_cnn_b[:,:,1]\n",
    "X_val_lstm2_a = X_intera_val_2[:,:,np.newaxis]\n",
    "X_val_lstm2_b = X_interb_val_2[:,:,np.newaxis]\n",
    "\n",
    "X_intera_test_2 = X_test_cnn_a[:,:,1]\n",
    "X_interb_test_2 = X_test_cnn_b[:,:,1]\n",
    "X_test_lstm2_a = X_intera_test_2[:,:,np.newaxis]\n",
    "X_test_lstm2_b = X_interb_test_2[:,:,np.newaxis]\n",
    "\n",
    "# Set for LSTM3\n",
    "\n",
    "X_intera_train_3 = X_train_cnn_a[:,:,2]\n",
    "X_interb_train_3 = X_train_cnn_b[:,:,2]\n",
    "X_train_lstm3_a = X_intera_train_3[:,:,np.newaxis]\n",
    "X_train_lstm3_b = X_interb_train_3[:,:,np.newaxis]\n",
    "\n",
    "X_intera_val_3 = X_val_cnn_a[:,:,2]\n",
    "X_interb_val_3 = X_val_cnn_b[:,:,2]\n",
    "X_val_lstm3_a = X_intera_val_3[:,:,np.newaxis]\n",
    "X_val_lstm3_b = X_interb_val_3[:,:,np.newaxis]\n",
    "\n",
    "X_intera_test_3 = X_test_cnn_a[:,:,2]\n",
    "X_interb_test_3 = X_test_cnn_b[:,:,2]\n",
    "X_test_lstm3_a = X_intera_test_3[:,:,np.newaxis]\n",
    "X_test_lstm3_b = X_interb_test_3[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "# Test Set for LSTM4 BERT\n",
    "\n",
    "X_intera_train_4 = bert_q1[:num_train]\n",
    "X_train_lstm4_a = X_intera_train_4[:,:,np.newaxis]\n",
    "\n",
    "X_intera_val_4 = bert_q1[num_train:num_val]\n",
    "X_val_lstm4_a = X_intera_val_4[:,:,np.newaxis]\n",
    "\n",
    "X_intera_test_4 = bert_q1[num_val:]\n",
    "X_test_lstm4_a = X_intera_test_4[:,:,np.newaxis]\n",
    "\n",
    "X_interb_train_4 = bert_q2[:num_train]\n",
    "X_train_lstm4_b = X_interb_train_4[:,:,np.newaxis]\n",
    "\n",
    "X_interb_val_4 = bert_q2[num_train:num_val]\n",
    "X_val_lstm4_b = X_interb_val_4[:,:,np.newaxis]\n",
    "\n",
    "X_interb_test_4 = bert_q2[num_val:]\n",
    "X_test_lstm4_b = X_interb_test_4[:,:,np.newaxis]\n",
    "\n",
    "print(\"Input Shapes\")\n",
    "print(\"CNN Shape\")\n",
    "print(X_train_cnn_a.shape,X_val_cnn_a.shape,X_test_cnn_a.shape)\n",
    "print(\"LSTM (x3) Shape:\")\n",
    "print(X_train_lstm4_a.shape,X_val_lstm4_a.shape,X_test_lstm4_a.shape)\n",
    "\n",
    "print(\"Features shape:\",features_train.shape,features_val.shape,features_test.shape)\n",
    "print(\"BERT Features shape:\",features_b_train.shape,features_b_val.shape,features_b_test.shape)\n",
    "\n",
    "print(\"Labels Shape\")\n",
    "print(Y_train.shape,Y_val.shape,Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./QQP_{epoch:02d}_{val_loss:.4f}.h5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, \n",
    "                                    monitor='val_loss', \n",
    "                                    verbose=0, \n",
    "                                    save_best_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# if sys.argv[3] == \"train\":\n",
    "# for epoch in range(1):\n",
    "#     if sys.argv[4] == \"resume\":\n",
    "#     print('Resuming Model..')\n",
    "#     net = load_model(sys.argv[5])\n",
    "#     #Add new config to trained model\n",
    "\n",
    "\n",
    "\n",
    "  # net.fit([X_train_cnn_a, X_train_cnn_b, X_train_lstm4_a, X_train_lstm4_b,features_train,features_b_train], \n",
    "  #           Y_train,\n",
    "  #         validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm4_a, X_val_lstm4_b,features_val,features_b_val]\n",
    "  #                         , Y_val),\n",
    "  #         batch_size=384, nb_epoch=1, shuffle=True,callbacks = callbacks_list)\n",
    "\n",
    "net.fit([ X_train_cnn_a, X_train_cnn_b,X_train_lstm1_a, X_train_lstm1_b,\n",
    "            X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b,X_train_lstm4_a, X_train_lstm4_b,features_train,features_b_train], \n",
    "            Y_train,\n",
    "          validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
    "                          X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b,X_val_lstm4_a, X_val_lstm4_b,features_val,features_b_val]\n",
    "                          , Y_val),\n",
    "          batch_size=384, nb_epoch=16, shuffle=True)\n",
    "\n",
    "  # net.fit([X_train_cnn_a, X_train_cnn_b, X_train_lstm1_a, X_train_lstm1_b,\n",
    "  #           X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b,features_train,features_b_train], \n",
    "  #           Y_train,\n",
    "  #         validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
    "  #                         X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b,features_val,features_b_val]\n",
    "  #                         , Y_val),\n",
    "  #         batch_size=384, nb_epoch=16, shuffle=True)\n",
    "# score = net.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm4_a, X_test_lstm4_b,features_test,features_b_test],Y_test,batch_size=384)\n",
    "score = net.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
    "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,X_test_lstm4_a, X_test_lstm4_b,features_test,features_b_test],Y_test,batch_size=384)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))\n",
    "else:\n",
    "net = load_model('QQP_07_0.6652.h5')\n",
    "score = net.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
    "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,features_test,features_b_test],Y_test,batch_size=384)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))\n",
    "return 0\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
