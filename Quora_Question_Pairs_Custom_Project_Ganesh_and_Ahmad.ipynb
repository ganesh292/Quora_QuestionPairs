{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora Question Pairs Custom Project- Ganesh and Ahmad.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jagij11_0SU",
        "colab_type": "text"
      },
      "source": [
        "# Setup-Google Credentials to Download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9T9uS4J_46Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgS90tGG_wPg",
        "colab_type": "text"
      },
      "source": [
        "#Download Embeddings and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPOUDZWJ1Ci4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download all Required Files using Pydrive\n",
        "from numpy import genfromtxt\n",
        "link='https://drive.google.com/open?id=1qo15b31PARBro7vL3K4wUAhqKaB1D8m_'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('data.csv')  \n",
        "\n",
        "df_sub = pd.read_csv('data.csv')\n",
        "\n",
        "#Download all Required Files using Pydrive\n",
        "link='https://drive.google.com/open?id=1ygb657QqsYKYt1oVSM52qXAYb6V14sJe'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('quora_features_BERT.csv')\n",
        "features = pd.read_csv('quora_features_BERT.csv')\n",
        "\n",
        "\n",
        "#Embeddings3*2\n",
        "link='https://drive.google.com/open?id=1OUMTHgF5CkQcIEwNfxRmRE8R4XOpYlTs'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ber_qpair_balanced.csv')\n",
        "features = genfromtxt(\"ber_qpair_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=18oaVUqneDcZ7Qf5zdhhazujFs4fVaENS'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fastext_q1_balanced.csv')\n",
        "ft_emb_q1  = genfromtxt(\"fastext_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=10N03ESlX0NM9UDzbrMxSTuGIEzyBZ16Z'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fastext_q2_balanced.csv')\n",
        "ft_emb_q2  = genfromtxt(\"fastext_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=13yUPFl9FDRORXCGrPpnGsTKF5Vcn-HSb'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove_q1_balanced.csv')\n",
        "glove_emb_q1 = genfromtxt(\"glove_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=1lS0jQA7ywuEK54Q-5FAXpmPYK_LBfira'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove_q2_balanced.csv')\n",
        "glove_emb_q2 = genfromtxt(\"glove_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=1E50i08MJXcMTV9iC7_OggCaBVWF0uq9_'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('w2vec_q1_balanced.csv')\n",
        "w2v_emb_q1 = genfromtxt(\"w2vec_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=1u83-aaKQVXDIo-elNlg9vnHmo1Q63YTF'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('w2vec_q2_balanced.csv')\n",
        "w2v_emb_q2 = genfromtxt(\"w2vec_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsNb1Sn1Eq5",
        "colab_type": "text"
      },
      "source": [
        "# Load Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gHIQsB81IYS",
        "colab_type": "code",
        "outputId": "102ee65c-e8a8-4ccd-ce15-f3d46f0c4c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import sys\n",
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "# from bert_serving.client import BertClient\n",
        "\n",
        "from numpy import genfromtxt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n",
        "# from fse.models import Sentence2Vec\n",
        "# # Make sure, that the fast version of fse is available!\n",
        "# from fse.models.sentence2vec import CY_ROUTINES\n",
        "# assert CY_ROUTINES\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras import callbacks\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# import tensorflow_hub as hub\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers, Add, concatenate, Layer,Lambda\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "\n",
        "# Network Architecture\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv1D , MaxPooling1D, Flatten,Dense,Input,Lambda\n",
        "from keras.layers import LSTM, Concatenate, Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras import backend as K\n",
        "# import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "# from models import InferSent\n",
        "from keras import regularizers"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYL1Fye65Rjd",
        "colab_type": "text"
      },
      "source": [
        "# Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFqVAoxE5TbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_base_network_cnn(input_dimensions):\n",
        "\n",
        "    input  = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    conv1  = Conv1D(filters=32,kernel_size=8,strides=1,activation = 'relu',name='conv1')(input)\n",
        "    b1 = BatchNormalization()(conv1)\n",
        "    d1 = Dropout(0.1)(b1)\n",
        "    \n",
        "    pool1  = MaxPooling1D(pool_size=1,strides=1,name='pool1')(d1)\n",
        "    d2  = Dropout(0.1)(pool1)\n",
        "    \n",
        "    conv2  = Conv1D(filters=64,kernel_size=6,strides=1,activation = 'relu',name='conv2')(d2)\n",
        "    b2 = BatchNormalization()(conv2)\n",
        "    d3 = Dropout(0.1)(b2)\n",
        "    \n",
        "    pool2  = MaxPooling1D(pool_size=1,strides=1,name='pool2')(d3)\n",
        "    d4 = Dropout(0.1)(pool2)\n",
        "    \n",
        "    conv3  = Conv1D(filters=128,kernel_size=4,strides=1,activation = 'relu',name='conv3')(d4)\n",
        "    b3 = BatchNormalization()(conv3)\n",
        "    d4 = Dropout(0.1)(b3)\n",
        "    \n",
        "    \n",
        "    pool3  = MaxPooling1D(pool_size=1,strides=1,name='pool3')(d4)\n",
        "    d5 = Dropout(0.1)(pool3)\n",
        "    \n",
        "    flat   = Flatten(name='flat_cnn')(d5)\n",
        "    d1 = Dense(100, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(flat)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(25,kernel_regularizer=regularizers.l2(0.01))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d2 = Dense(5,kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "    drop3 = Dropout(0.1)(d2)\n",
        "    bn = BatchNormalization()(drop3)\n",
        "\n",
        "    model  = Model(input=input,output=bn)\n",
        "  \n",
        "  \n",
        "    return model\n",
        "\n",
        "\n",
        "def dense_network(features1):\n",
        "    input = Input(shape=(features1[0],))\n",
        "    #x = Flatten()(features)\n",
        "    d1 = Dense(100, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(input)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(25,kernel_regularizer=regularizers.l2(0.01))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d2 = Dense(5,kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "    drop3 = Dropout(0.1)(d2)\n",
        "    b3 = BatchNormalization()(drop3)\n",
        "#     flat   = Flatten(name='flat_dnn2')(b3)\n",
        "    model = Model(input = input,output=b3)\n",
        "    return model\n",
        "  \n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "\n",
        "def create_network(input_dimensions,num_features):\n",
        "\n",
        "#     # #Fasttext\n",
        "    base_network_lstm_1 = dense_network([324,1])\n",
        "    input_a_lstm_1 = Input(shape=(input_dimensions[0],))\n",
        "    input_b_lstm_1 = Input(shape=(input_dimensions[0],))\n",
        "    # LSTM with embedding 1\n",
        "    inter_a_lstm_1 = base_network_lstm_1(input_a_lstm_1)\n",
        "    inter_b_lstm_1 = base_network_lstm_1(input_b_lstm_1)\n",
        "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
        "\n",
        "\n",
        "    #W2V\n",
        "    base_network_lstm_2 = dense_network([324,1])\n",
        "    input_a_lstm_2 = Input(shape=(input_dimensions[0],))\n",
        "    input_b_lstm_2 = Input(shape=(input_dimensions[0],))\n",
        "    # LSTM with embedding 2\n",
        "    inter_a_lstm_2 = base_network_lstm_2(input_a_lstm_2)\n",
        "    inter_b_lstm_2 = base_network_lstm_2(input_b_lstm_2)\n",
        "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
        "\n",
        "\n",
        "    #Glove\n",
        "    base_network_lstm_3 = create_base_network_lstm([324,1])\n",
        "    input_a_lstm_3 = Input(shape=(input_dimensions[0],1))\n",
        "    input_b_lstm_3 = Input(shape=(input_dimensions[0],1))\n",
        "    # LSTM with embedding 3\n",
        "    inter_a_lstm_3 = base_network_lstm_3(input_a_lstm_3)\n",
        "    inter_b_lstm_3 = base_network_lstm_3(input_b_lstm_3)\n",
        "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
        "\n",
        "    #Uncomment to Use BERT as Siamese as well\n",
        "#     base_network_lstm_4 = dense_network([768,1])\n",
        "#     input_a_lstm_4 = Input(shape=(768,))\n",
        "#     input_b_lstm_4 = Input(shape=(768,))\n",
        "#     # LSTM with embedding 3\n",
        "#     inter_a_lstm_4 = base_network_lstm_4(input_a_lstm_4)\n",
        "#     inter_b_lstm_4 = base_network_lstm_4(input_b_lstm_4)\n",
        "\n",
        "\n",
        "#     CNN\n",
        "    base_network_cnn = create_base_network_cnn(input_dimensions)\n",
        "    # CNN with 3 channel embedding\n",
        "    input_a_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    input_b_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    inter_a_cnn = base_network_cnn(input_a_cnn)\n",
        "    inter_b_cnn = base_network_cnn(input_b_cnn)\n",
        "\n",
        "\n",
        "\n",
        "    d_cnn = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_cnn, inter_b_cnn])\n",
        "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
        "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
        "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
        "#     d_lstm_4 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_4, inter_b_lstm_4])\n",
        "\n",
        "    # Additional Features from (BERT)\n",
        "    features = Input(shape=(num_features,))\n",
        "\n",
        "    #BERT itself\n",
        "    features_b = Input(shape=(768,))\n",
        "\n",
        "\n",
        "    #Concatenation of Features\n",
        "    feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_1,d_lstm_2,d_lstm_3,features,features_b])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #x = Flatten()(features)\n",
        "    d1 = Dense(300, activation='relu',kernel_regularizer=regularizers.l2(0.01))(feature_set)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(20, activation='relu',kernel_regularizer=regularizers.l2(0.001))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d3 = Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "\n",
        "\n",
        "#     model = Model(input=[feature_set,features_b], output=d3)\n",
        "    model = Model(input=[input_a_cnn, input_b_cnn , input_a_lstm_1, input_b_lstm_1, input_a_lstm_2, input_b_lstm_2, input_a_lstm_3, input_b_lstm_3,features,features_b], output=d3)  \n",
        "    print(\"Model Architecture Designed\")\n",
        "    return model\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIxjut-O6Ydm",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Embeddings from Downloaded files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUQ5m4ba6eYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sub['question1'] = df_sub['question1'].apply(lambda x: str(x))\n",
        "df_sub['question2'] = df_sub['question2'].apply(lambda x: str(x))\n",
        "q1sents = list(df_sub['question1'])\n",
        "q2sents = list(df_sub['question2'])\n",
        "tokenized_q1sents = [word_tokenize(i) for i in list(df_sub['question1'])]\n",
        "tokenized_q2sents = [word_tokenize(i) for i in list(df_sub['question2'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAgp5wPd6gr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_emb_q1 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/word2vec/w2vec_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "w2v_emb_q2 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/word2vec/w2vec_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "w2v_emb_q1 = np.delete(w2v_emb_q1, 0, 1)\n",
        "w2v_emb_q2 = np.delete(w2v_emb_q2, 0, 1)\n",
        "print('Loading Embeddings fastext')\n",
        "ft_emb_q1 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/fastext/fastext_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "ft_emb_q2 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/fastext/fastext_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "ft_emb_q1 = np.delete(ft_emb_q1, 0, 1)\n",
        "ft_emb_q2 = np.delete(ft_emb_q2,0, 1)\n",
        "print('Loading Embeddings glove')\n",
        "glove_emb_q1 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/glove/glove_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "glove_emb_q2 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/glove/glove_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "glove_emb_q1 = np.delete(glove_emb_q1,0, 1)\n",
        "glove_emb_q2 = np.delete(glove_emb_q2, 0, 1)\n",
        "print('Loading Embeddings BERT')\n",
        "bert_q = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_qpair_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "bert_q = np.delete(bert_q,0,1)\n",
        "bert_q1 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_q1_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "bert_q1 = np.delete(bert_q1,0,1)\n",
        "print('Loading Embeddings BERTQ2')\n",
        "bert_q2 = genfromtxt('./drive/My Drive/Ganesh_MSCI/Unbalanced_Embeddings/bert/bert_q2_unbalanced.csv', delimiter=',',skip_header=1)\n",
        "bert_q2 = np.delete(bert_q2,0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGI1etv57Uqh",
        "colab_type": "text"
      },
      "source": [
        "# Prepare-Train-Test-Validation Data for Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXKplIhu7AcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5159d672-7928-48ff-ac09-7ed9af6898e4"
      },
      "source": [
        "num_train = int(df_sub.shape[0] * 0.70)\n",
        "num_val = int(df_sub.shape[0] * 0.10)\n",
        "num_test = df_sub.shape[0] - num_train - num_val \n",
        "\n",
        "print(\"Number of training pairs: %i\"%(num_train))\n",
        "print(\"Number of Validation pairs: %i\"%(num_val))\n",
        "print(\"Number of testing pairs: %i\"%(num_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training pairs: 28299\n",
            "Number of Validation pairs: 4042\n",
            "Number of testing pairs: 8087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdqODaB7Cej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "09e142da-6010-4887-dc10-92265af6819a"
      },
      "source": [
        "# init data data arrays\n",
        "X_train_cnn_a = np.zeros([num_train, 324, 3])\n",
        "X_test_cnn_a  = np.zeros([num_test, 324, 3])\n",
        "X_val_cnn_a  = np.zeros([num_val, 324, 3])\n",
        "\n",
        "X_train_cnn_b = np.zeros([num_train, 324, 3])\n",
        "X_test_cnn_b  = np.zeros([num_test, 324, 3])\n",
        "X_val_cnn_b  = np.zeros([num_val, 324, 3])\n",
        "\n",
        "Y_train = np.zeros([num_train]) \n",
        "Y_test = np.zeros([num_test])\n",
        "Y_val = np.zeros([num_val]) \n",
        "\n",
        "\n",
        "#Labels\n",
        "Y_train = df_sub['is_duplicate'].values[num_train]\n",
        "Y_val = df_sub['is_duplicate'].values[num_val]\n",
        "Y_test = df_sub['is_duplicate'].values[num_val]\n",
        "\n",
        "\n",
        "\n",
        "num_val = num_train + int(df_sub.shape[0] * 0.10)\n",
        "# fill data arrays with features\n",
        "X_train_cnn_a[:,:,0] = ft_emb_q1[:num_train]\n",
        "X_train_cnn_a[:,:,1] = w2v_emb_q1[:num_train]\n",
        "X_train_cnn_a[:,:,2] = glove_emb_q1[:num_train]\n",
        "# X_train_cnn_a[:,:,3] = bert_q1_pca[:num_train]\n",
        "\n",
        "X_train_cnn_b[:,:,0] = ft_emb_q2[:num_train]\n",
        "X_train_cnn_b[:,:,1] = w2v_emb_q2[:num_train]\n",
        "X_train_cnn_b[:,:,2] = glove_emb_q2[:num_train]\n",
        "# X_train_cnn_b[:,:,3] = bert_q2_pca[:num_train]\n",
        "\n",
        "features_train = features[:num_train]\n",
        "features_b_train = bert_q[:num_train]\n",
        "Y_train = df_sub[:num_train]['is_duplicate'].values\n",
        "\n",
        "X_val_cnn_a[:,:,0] = ft_emb_q1[num_train:num_val]\n",
        "X_val_cnn_a[:,:,1] = w2v_emb_q1[num_train:num_val]\n",
        "X_val_cnn_a[:,:,2] = glove_emb_q1[num_train:num_val]\n",
        "# X_val_cnn_a[:,:,3] = bert_q1_pca[num_train:num_val]\n",
        "\n",
        "X_val_cnn_b[:,:,0] = ft_emb_q2[num_train:num_val]\n",
        "X_val_cnn_b[:,:,1] = w2v_emb_q2[num_train:num_val]\n",
        "X_val_cnn_b[:,:,2] = glove_emb_q2[num_train:num_val]\n",
        "# X_val_cnn_b[:,:,3] = bert_q2_pca[num_train:num_val]\n",
        "\n",
        "features_val = features[num_train:num_val]\n",
        "features_b_val = bert_q[num_train:num_val]\n",
        "Y_val = df_sub[num_train:num_val]['is_duplicate'].values\n",
        "\n",
        "\n",
        "X_test_cnn_a[:,:,0] = ft_emb_q1[num_val:]\n",
        "X_test_cnn_a[:,:,1] = w2v_emb_q1[num_val:]\n",
        "X_test_cnn_a[:,:,2] = glove_emb_q1[num_val:]\n",
        "# X_test_cnn_a[:,:,3] = bert_q1_pca[num_val:]\n",
        "\n",
        "X_test_cnn_b[:,:,0] = ft_emb_q2[num_val:]\n",
        "X_test_cnn_b[:,:,1] = w2v_emb_q2[num_val:]\n",
        "X_test_cnn_b[:,:,2] = glove_emb_q2[num_val:]\n",
        "# X_test_cnn_b[:,:,3] = bert_q2_pca[num_val:]\n",
        "\n",
        "\n",
        "features_test = features[num_val:]\n",
        "features_b_test = bert_q[num_val:]\n",
        "Y_test = df_sub[num_val:]['is_duplicate'].values\n",
        "\n",
        "\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes=2)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes=2)\n",
        "Y_val = keras.utils.to_categorical(Y_val, num_classes=2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8dec48da3c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnum_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# fill data arrays with features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mX_train_cnn_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_emb_q1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mX_train_cnn_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_emb_q1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mX_train_cnn_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_emb_q1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (28299,325) into shape (28299,324)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xETTzV37HLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Training Sets for Siamese1\n",
        "X_intera_train_1 = X_train_cnn_a[:,:,0]\n",
        "X_interb_train_1 = X_train_cnn_b[:,:,0]\n",
        "X_train_lstm1_a = X_intera_train_1\n",
        "X_train_lstm1_b = X_interb_train_1\n",
        "\n",
        "X_intera_val_1 = X_val_cnn_a[:,:,0]\n",
        "X_interb_val_1 = X_val_cnn_b[:,:,0]\n",
        "X_val_lstm1_a = X_intera_val_1\n",
        "X_val_lstm1_b = X_interb_val_1\n",
        "\n",
        "X_intera_test_1 = X_test_cnn_a[:,:,0]\n",
        "X_interb_test_1 = X_test_cnn_b[:,:,0]\n",
        "X_test_lstm1_a = X_intera_test_1\n",
        "X_test_lstm1_b = X_interb_test_1\n",
        "\n",
        "\n",
        "# Sets for Siamese2\n",
        "X_intera_train_2 = X_train_cnn_a[:,:,1]\n",
        "X_interb_train_2 = X_train_cnn_b[:,:,1]\n",
        "X_train_lstm2_a = X_intera_train_2\n",
        "X_train_lstm2_b = X_interb_train_2\n",
        "\n",
        "X_intera_val_2 = X_val_cnn_a[:,:,1]\n",
        "X_interb_val_2 = X_val_cnn_b[:,:,1]\n",
        "X_val_lstm2_a = X_intera_val_2\n",
        "X_val_lstm2_b = X_interb_val_2\n",
        "\n",
        "X_intera_test_2 = X_test_cnn_a[:,:,1]\n",
        "X_interb_test_2 = X_test_cnn_b[:,:,1]\n",
        "X_test_lstm2_a = X_intera_test_2\n",
        "X_test_lstm2_b = X_interb_test_2\n",
        "\n",
        "# Set for Siamese3\n",
        "\n",
        "X_intera_train_3 = X_train_cnn_a[:,:,2]\n",
        "X_interb_train_3 = X_train_cnn_b[:,:,2]\n",
        "X_train_lstm3_a = X_intera_train_3\n",
        "X_train_lstm3_b = X_interb_train_3\n",
        "\n",
        "X_intera_val_3 = X_val_cnn_a[:,:,2]\n",
        "X_interb_val_3 = X_val_cnn_b[:,:,2]\n",
        "X_val_lstm3_a = X_intera_val_3\n",
        "X_val_lstm3_b = X_interb_val_3\n",
        "X_intera_test_3 = X_test_cnn_a[:,:,2]\n",
        "X_interb_test_3 = X_test_cnn_b[:,:,2]\n",
        "X_test_lstm3_a = X_intera_test_3\n",
        "X_test_lstm3_b = X_interb_test_3\n",
        "\n",
        "\n",
        "# Test Set for LSTM4 BERT\n",
        "\n",
        "X_intera_train_4 = bert_q1[:num_train]\n",
        "X_train_lstm4_a = X_intera_train_4\n",
        "\n",
        "X_intera_val_4 = bert_q1[num_train:num_val]\n",
        "X_val_lstm4_a = X_intera_val_4\n",
        "\n",
        "X_intera_test_4 = bert_q1[num_val:]\n",
        "X_test_lstm4_a = X_intera_test_4\n",
        "\n",
        "X_interb_train_4 = bert_q2[:num_train]\n",
        "X_train_lstm4_b = X_interb_train_4\n",
        "\n",
        "X_interb_val_4 = bert_q2[num_train:num_val]\n",
        "X_val_lstm4_b = X_interb_val_4\n",
        "\n",
        "X_interb_test_4 = bert_q2[num_val:]\n",
        "X_test_lstm4_b = X_interb_test_4\n",
        "# X_test_lstm4_b = X_interb_test_4\n",
        "\n",
        "print(\"Input Shapes\")\n",
        "print(\"CNN Shape\")\n",
        "print(X_train_cnn_a.shape,X_val_cnn_a.shape,X_test_cnn_a.shape)\n",
        "print(\"LSTM (x3) Shape:\")\n",
        "print(X_train_lstm1_a.shape,X_val_lstm1_a.shape,X_test_lstm1_a.shape)\n",
        "print(\"LSTM (BERT) Shape:\")\n",
        "print(X_train_lstm4_a.shape,X_val_lstm4_a.shape,X_test_lstm4_a.shape)\n",
        "\n",
        "print(\"Features shape:\",features_train.shape,features_val.shape,features_test.shape)\n",
        "print(\"BERT Features shape:\",features_b_train.shape,features_b_val.shape,features_b_test.shape)\n",
        "\n",
        "print(\"Labels Shape\")\n",
        "print(Y_train.shape,Y_val.shape,Y_test.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFTgzbX67aHM",
        "colab_type": "text"
      },
      "source": [
        "# Training with DNN as Classification Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpHsSI3R7gt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = create_network([324,3],25)\n",
        "optimizer = Adam(lr=0.001)\n",
        "net.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=['accuracy'])\n",
        "print(net.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZZOS5O7hd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"./Model7_{epoch:02d}_{val_acc:.4f}.h5\"\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath, \n",
        "                                    monitor='val_acc', \n",
        "                                    verbose=0, \n",
        "                                    save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "net.fit([ X_train_cnn_a, X_train_cnn_b,X_train_lstm1_a, X_train_lstm1_b,\n",
        "            X_train_lstm2_a, X_train_lstm2_b,features_train,features_b_train], \n",
        "            Y_train,\n",
        "          validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
        "                          X_val_lstm2_a, X_val_lstm2_b,features_val,features_b_val]\n",
        "                          , Y_val),\n",
        "          batch_size=384, nb_epoch=20, shuffle=True,callbacks = callbacks_list)\n",
        "\n",
        "score = net.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
        "              X_test_lstm2_a, X_test_lstm2_b,features_test,features_b_test],Y_test,batch_size=384)\n",
        "\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gKYW5v47k9k",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Best Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bc1lC-N7hby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_old = Y_train\n",
        "Y_test_old = Y_test\n",
        "Y_val_old = Y_val\n",
        "\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes=2)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes=2)\n",
        "Y_val = keras.utils.to_categorical(Y_val, num_classes=2)\n",
        "\n",
        "# Getting features for classical machine learning algorithms\n",
        "def feature_extractor():\n",
        "  intermediate_layer_model = Model(inputs=net.input,\n",
        "                                 outputs=net.layers[-8].output)\n",
        "  data_train = [ X_train_cnn_a, X_train_cnn_b,X_train_lstm1_a, X_train_lstm1_b,\n",
        "            X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b,features_train,features_b_train]\n",
        "\n",
        "  data_validation = [X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
        "                          X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b,features_val,features_b_val]\n",
        "\n",
        "  data_test = [X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
        "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,features_test,features_b_test]\n",
        "\n",
        "\n",
        "  intermediate_output_train = intermediate_layer_model.predict(data_train)\n",
        "  intermediate_output_validation = intermediate_layer_model.predict(data_validation)\n",
        "  intermediate_output_test = intermediate_layer_model.predict(data_test)\n",
        "  \n",
        "  features_combined = [intermediate_output_train, intermediate_output_validation,intermediate_output_test]\n",
        "  return features_combined\n",
        "\n",
        "[feats_train,feats_val,feats_test] = feature_extractor()\n",
        "\n",
        "# Getting data for classical machine learning algorithms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "train_feats = np.array(list(feats_train) + list(feats_val))\n",
        "train_Y     = np.array(list(Y_train_old) + list(Y_val_old))\n",
        "\n",
        "test_feats = feats_test\n",
        "test_Y = Y_test_old\n",
        "\n",
        "print(\"train_feats :\",train_feats.shape)\n",
        "print(\"train_Y : \",train_Y.shape)\n",
        "\n",
        "print(\"test_feats : \",test_feats.shape)\n",
        "print(\"test_Y : \",test_Y.shape)\n",
        "\n",
        "# Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "svm_clf = SVC(gamma='auto')\n",
        "svm_clf.fit(train_feats,train_Y)\n",
        "print(svm_clf.score(test_feats,test_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6vlIPgy7oce",
        "colab_type": "text"
      },
      "source": [
        "# Classification with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ElyZIE7haA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmDLd16G7sio",
        "colab_type": "text"
      },
      "source": [
        "# Classification with XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTzwukQ7vVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}